1.Scenario: To identify potential bottlenecks by analyzing the performance of servers, you want to determine the 25th, 50th, and 75th 
percentiles of response times.

CODE:
import numpy as np

# Response times data
response_times = np.array([23, 12, 45, 19, 28, 34, 41, 50, 67, 82, 93, 11, 25, 36, 49])

# Calculate percentiles
p25 = np.percentile(response_times, 25)
p50 = np.percentile(response_times, 50)
p75 = np.percentile(response_times, 75)

print(f"25th percentile: {p25}")
print(f"50th percentile (median): {p50}")
print(f"75th percentile: {p75}")

ALGORITHM:
Here are the 4 points of the algorithm for the above code:

Point 1: Data Collection

- Collect response times data from servers
- Store data in a NumPy array (e.g., response_times)

Point 2: Percentile Calculation

- Use np.percentile() function to calculate:
    - 25th percentile (p25)
    - 50th percentile (p50, also known as the median)
    - 75th percentile (p75)

Point 3: Interpolation (if necessary)

- If the percentile falls between two data points, np.percentile() will interpolate the value
- Interpolation is done to ensure accurate percentile values

Point 4: Result

- Print or store the calculated percentile values (p25, p50, p75)
- Use these values to identify potential bottlenecks in server performance

By following these 4 points, you can effectively calculate and analyze the 25th, 50th, and 75th percentiles of response times to identify potential bottlenecks in server performance.

OUTPUT:
25th percentile: 24.0
50th percentile (median): 36.0
75th percentile: 49.5

2.Scenario: In a medical study, you have collected data on patients  recovery times after a procedure. 
Calculate the 10th, 50th, and 90th percentiles to understand the distribution of recovery times.

CODE:
import numpy as np

# Recovery times data
recovery_times = np.array([12, 15, 18, 20, 22, 25, 28, 30, 35, 40, 45, 50, 55, 60])

# Calculate percentiles
p10 = np.percentile(recovery_times, 10)
p50 = np.percentile(recovery_times, 50)
p90 = np.percentile(recovery_times, 90)

print(f"10th percentile: {p10}")
print(f"50th percentile (median): {p50}")
print(f"90th percentile: {p90}")

ALGORITHM:
Here are the 4 points of the algorithm to calculate the 10th, 50th, and 90th percentiles of recovery times:

Point 1: Data Collection

- Collect recovery times data from patients
- Store data in a NumPy array (e.g., recovery_times)

Point 2: Percentile Calculation

- Use np.percentile() function to calculate:
    - 10th percentile (p10)
    - 50th percentile (p50, also known as the median)
    - 90th percentile (p90)

Point 3: Interpolation (if necessary)

- If the percentile falls between two data points, np.percentile() will interpolate the value
- Interpolation is done to ensure accurate percentile values

Point 4: Result

- Print or store the calculated percentile values (p10, p50, p90)
- Use these values to understand the distribution of recovery times and identify patterns or outliers in the data

By following these 4 points, you can effectively calculate and analyze the 10th, 50th, and 90th percentiles of recovery times to gain insights into patient recovery patterns.

OUTPUT:
10th percentile: 15.9
50th percentile (median): 29.0
90th percentile: 53.50000000000001

3.You are working with an e-commerce company that has collected data on the purchase
amounts made by customers over the past month. The dataset includes the purchase
amounts (in dollars) for each transaction. Utilize measures of central tendency to answer
the following questions:
● Calculate the mean (average) purchase amount to understand the typical
spending behavior of customers.
● Identify the mode of the purchase amounts to find the most frequently
occurring purchase amount, helping the company understand popular spending
levels

CODE:
import numpy as np

# Purchase amounts dataset
purchase_amounts = np.array([20, 30, 40, 50, 60, 70, 80, 90, 100, 20, 30, 40, 50, 60])

# Calculate mean (average) purchase amount
mean_purchase_amount = np.mean(purchase_amounts)
print(f"Mean Purchase Amount: ${mean_purchase_amount:.2f}")

# Calculate mode of purchase amounts
frequency_distribution = np.bincount(purchase_amounts)
mode_purchase_amount = np.argmax(frequency_distribution)
print(f"Mode Purchase Amount: ${mode_purchase_amount}")

ALGORITHM:
Here are the 4 points of the algorithm to calculate the mean and mode of purchase amounts:

Point 1: Data Collection

- Collect purchase amounts data from transactions
- Store data in a NumPy array (e.g., purchase_amounts)

Point 2: Mean Calculation

- Calculate the sum of all purchase amounts using np.sum()
- Count the total number of transactions using len()
- Divide the sum by the count to get the mean using np.mean()

Point 3: Mode Calculation

- Create a frequency distribution of purchase amounts using np.bincount()
- Identify the purchase amount with the highest frequency using np.argmax()

Point 4: Result

- Print or store the calculated mean and mode values
- Use these values to understand typical spending behavior and popular spending levels, informing business decisions

By following these 4 points, you can effectively calculate and analyze the mean and mode of purchase amounts to gain insights into customer spending patterns.

OUTPUT:
Mean Purchase Amount: $52.86
Mode Purchase Amount: $20

4.Scenario: You are dealing with a dataset containing the monthly expenses of different
departments in a company. Use NumPy functions to efficiently calculate both the variance
and covariance matrix of these expenses.

CODE:
import numpy as np

# Load dataset into a 2D array
expenses = np.array([
    [100, 120, 110, 130],  # Department 1
    [200, 220, 210, 230],  # Department 2
    [300, 320, 310, 330]   # Department 3
])

# Calculate variance of each department's expenses
variance = np.var(expenses, axis=0)
print("Variance:", variance)

# Calculate covariance matrix of expenses
covariance_matrix = np.cov(expenses)
print("Covariance Matrix:\n", covariance_matrix)

ALGORITHM:
Here are the 4 points of the algorithm to calculate the variance and covariance matrix of monthly expenses:

Point 1: Data Loading

- Import NumPy library
- Load dataset into a 2D array (e.g., expenses), where each row represents a department and each column represents a month

Point 2: Variance Calculation

- Use np.var() to calculate the variance of each department's expenses along the columns (axis=0)
- Store the result in a variable (e.g., variance)

Point 3: Covariance Matrix Calculation

- Use np.cov() to calculate the covariance matrix of the expenses
- Store the result in a variable (e.g., covariance_matrix)

Point 4: Result

- Print or store the calculated variance and covariance_matrix values
- Use these values to analyze the variability and relationships between departmental expenses, informing budgeting and cost-control decisions

By following these 4 points, you can efficiently calculate and analyze the variance and covariance matrix of monthly expenses using NumPy.

OUTPUT:
Variance: [6666.66666667 6666.66666667 6666.66666667 6666.66666667]
Covariance Matrix:
 [[166.66666667 166.66666667 166.66666667]
 [166.66666667 166.66666667 166.66666667]
 [166.66666667 166.66666667 166.66666667]]

5.Scenario: You are investigating a dataset representing the daily temperatures in a city.
Calculate the variance and identify potential outliers that may indicate unusual weather
conditions.

CODE:
import numpy as np

# Load dataset into a 1D array
temperatures = np.array([22, 24, 25, 28, 30, 32, 35, 40, 45, 50])

# Calculate mean
mean_temp = np.mean(temperatures)
print("Mean Temperature:", mean_temp)

# Calculate variance
variance = np.var(temperatures)
print("Variance:", variance)

# Calculate standard deviation
std_dev = np.sqrt(variance)
print("Standard Deviation:", std_dev)

# Identify potential outliers
outliers = np.abs(temperatures - mean_temp) > 2 * std_dev
print("Potential Outliers:", temperatures[outliers])

ALGORITHM:
Here are the 4 points of the algorithm to calculate the variance and identify potential outliers:

Point 1: Data Loading

- Import NumPy library
- Load dataset into a 1D array (e.g., temperatures)

Point 2: Variance Calculation

- Calculate the mean of the temperatures using np.mean()
- Calculate the variance of the temperatures using np.var()
- Calculate the standard deviation using np.sqrt(variance)

Point 3: Outlier Identification

- Identify potential outliers by checking for temperatures more than 2-3 standard deviations away from the mean
- Use absolute difference to detect outliers on both sides of the mean

Point 4: Result

- Print or store the calculated variance, standard deviation, and potential outliers
- Analyze the results to understand temperature variability and identify unusual weather conditions

By following these 4 points, you can effectively calculate the variance and identify potential outliers in the temperature dataset, providing insights into weather patterns and anomalies.

OUTPUT:
Mean Temperature: 33.1
Variance: 78.69
Standard Deviation: 8.870738413458037
Potential Outliers: []

6.Write a python program will take in a dataset containing daily temperature readings for
each city over a year and perform the following tasks:
● Calculate the mean temperature for each city.
● Calculate the standard deviation of temperature for each city.
● Determine the city with the highest temperature range (difference between the
highest and lowest temperatures).
● Find the city with the most consistent temperature (the lowest standard deviation).

CODE:
import numpy as np

# Sample dataset: daily temperature readings for each city over a year
data = {
    'City A': np.array([22, 24, 25, 28, 30, 32, 35, 40, 45, 50, 20, 18, 19, 21, 23, 25, 27, 29, 31, 33, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100]),
    'City B': np.array([18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120]),
    'City C': np.array([15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99, 101, 103, 105, 107, 109, 111, 113, 115, 117, 119])
}

# Calculate mean temperature for each city
mean_temps = {city: np.mean(temps) for city, temps in data.items()}
print("Mean Temperatures:", mean_temps)

# Calculate standard deviation of temperature for each city
std_devs = {city: np.std(temps) for city, temps in data.items()}
print("Standard Deviations:", std_devs)

# Determine the city with the highest temperature range
temp_ranges = {city: np.ptp(temps) for city, temps in data.items()}
city_highest_range = max(temp_ranges, key=temp_ranges.get)
print("City with Highest Temperature Range:", city_highest_range)

# Find the city with the most consistent temperature (lowest standard deviation)
city_lowest_std_dev = min(std_devs, key=std_devs.get)
print("City with Most Consistent Temperature:", city_lowest_std_dev)


ALGORITHM:
Here are the 4 points of the algorithm to calculate the mean temperature, standard deviation, highest temperature range, and most consistent temperature:

Point 1: Data Loading

- Import NumPy library
- Load dataset into a dictionary (e.g., data) with city names as keys and temperature arrays as values

Point 2: Mean and Standard Deviation Calculation

- Calculate the mean temperature for each city using np.mean()
- Calculate the standard deviation of temperature for each city using np.std()

Point 3: Temperature Range and Consistency Analysis

- Calculate the temperature range for each city using np.ptp()
- Identify the city with the highest temperature range using max()
- Identify the city with the most consistent temperature (lowest standard deviation) using min()

Point 4: Result

- Print or store the calculated mean temperatures, standard deviations, city with highest temperature range, and city with most consistent temperature
- Analyze the results to understand temperature patterns and consistency across cities

By following these 4 points, you can effectively analyze the temperature dataset and gain insights into temperature trends and consistency.

OUTPUT:
Mean Temperatures: {'City A': 53.22641509433962, 'City B': 69.0, 'City C': 67.0}
Standard Deviations: {'City A': 24.74446117511078, 'City B': 30.01666203960727, 'City C': 30.59411708155671}
City with Highest Temperature Range: City C
City with Most Consistent Temperature: City A

7.Scenario: You are working with a dataset representing the daily sales of a product over the past month. 
Calculate the variance of the daily sales to understand how much the sales
figures deviate from the mean. 

CODE:
import numpy as np

# Load dataset into a numpy array
sales = np.array([25, 30, 28, 35, 40, 38, 42, 45, 50, 55, 60, 58, 62, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120])

# Calculate mean sales
mean_sales = np.mean(sales)
print("Mean Sales:", mean_sales)

# Calculate variance of sales
deviations = sales - mean_sales
squared_deviations = deviations ** 2
sum_squared_deviations = np.sum(squared_deviations)
variance = sum_squared_deviations / (len(sales) - 1)
print("Variance of Sales:", variance)

ALGORITHM:
Here are the 4 points of the algorithm to calculate the variance of daily sales:

Point 1: Data Loading

- Import NumPy library
- Load dataset into a NumPy array (e.g., sales)

Point 2: Mean Calculation

- Calculate the mean of the sales using np.mean()

Point 3: Variance Calculation

- Subtract the mean from each sales figure to find the deviations
- Square each deviation
- Calculate the sum of the squared deviations
- Divide the sum by the number of sales figures minus one (for sample variance)

Point 4: Result

- Print or store the calculated variance value
- Analyze the result to understand the spread or dispersion of the daily sales data

By following these 4 points, you can effectively calculate the variance of daily sales and gain insights into the consistency of sales performance.

OUTPUT:
Mean Sales: 67.12
Variance of Sales: 846.7766666666666















